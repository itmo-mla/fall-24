# Лабораторная работа №3 "Логическая классификация"

В рамках данной работы были реализованы алгоритмы ID3 дерева для классификации и регрессии с учётом наличия пропусков в данных. Чтобы дерево имело возможность работать с вещественными признаками, был написан генератор предикатов по исходным признакам. В работе дерево опирается на критерии ветвления, три из которых - критерий Донского, многоклассовый энтропийный критерий и критерий регрессии - были также реализованы в ходе работы. Кроме того в работе представлен код редукции обученного дерева для борьбы с переобучением.
Все комбинации параметров дерева и разновидностей алгоритма были протестированы на [данных о спортивных тренировках](https://www.kaggle.com/datasets/valakhorasani/gym-members-exercise-dataset).

## Данные

Набор состоит из 973 строк и 15 колонок. Данные содержат и категориальные (2), и вещественные (13) признаки. Пропусков в них нет. Пропуски были внесены вручную случайным образом в 750 ячейках набора данных. В дальнейшем алгоритмы будут рассматриваться на исходных данных без пропусков, на данных с пропусками и на данных, из которых были удалены строки с пропусками.

Для задачи классификации в качестве целевой переменной используется колонка `Experience_Level` - опыт спортсмена. В наборе данных представлено 3 значения `Experience_Level`: `1`, `2`, `3`. Баланса по целевой переменной нет. Для значения `1` в наборе представлено 396 строк, для значения `2` - 406, для значения `3` - 191.

Для регресии в качестве целевой переменной используется значение сожжённых за тренировку калорий из колонки `Calories_Burned`.

## ID3 - классификация

На задаче предсказания уровня опыта ID3 с критерием Донского с разбиением области значений вещественных признаков на 20 подотрезков показал точность (accuracy) в `0.807` на полных данных. При уменьшении количества подотрезков до 2 качество упало до `0.754`.

При использовании ID3 с критерием Донского и предикатами на 20 подотрезков на данных, содержащих пропуски, было получено качество в `0.537`. При использовании многоклассового энтропийного критерия качество упало до `0.410`. Это лучше случайного подбора или константного классификатора, но значительно хуже модели, полученной на данных без пропусков.

Значение точности ID3 с критерием Донского и предикатами на 20 подотрезков на данных, из которых были удалены все пропуски составило `0.773`. Количество строк данных при этом относительно полных данных упало в 2 раза (точность на полных данных при этом составила `0.807`).

**Выводы:**
- Увеличение количества подотрезков вещественных признаков ведёт к увеличению объёма информации в дереве. Это позволяет дереву точнее проводить классификацию
- Дерево чувствительно к пропускам. Наличие пропусков ухудшило качество сильнее, чем двухкратное уменьшение количества строк данных
- Дерево может эффективно учиться на небольшой подвыборке набора данных. Уменьшение обучающего набора данных в 2 раза уронило качество с `81%` до `77%` (то есть всего на `4%`)
- Согласно опытам, на рассматриваемых данных ID3 лучше работает с критерием Донского, чем с многоклассовым энтройпийным критерием

## ID3 - регрессия

При использовании ID3 дерева для решения задачи регресии с количеством подотрезков = 20 была получена ошибка MSE = `14819`. При уменьшении количества подотрезков до 2 ошибка вырастала до `62278`.

**Выводы:**
- Дерево для решения задачи регресии более чувствительно к количеству подотрезков для предикатов по вещественным признакам, чем дерево для классификации

## Редукция дерева

При использовании ID3 дерева для решения задачи классификации с критерием Донского и с количеством подотрезков = 2 (малое значение специально для избежания переобучения) на 65% исходного набора данных без пропусков была получена точность в `0.758`. После процедуры стрижки дерева точность повысилась до `0.773`.

**Выводы:**
- Стрижка дерева позволяет бороться с переобучением и увеличивает качество предсказаний
- Даже на небольшом количестве подотрезков дерево способно переобучиться

## Сравнение с библиотечной версией

Библиотечный аналог работает с вещественными признаками по своему внутреннему алгоритму, поэтому настроить величину, аналогичную количеству подотрезков, нет возможности.

Точность классификации алгоритма на исходном наборе данных без пропусков составила `0.861`, что немного выше точности реализованного выше решения (`0.807`). Однако при удалении строк с пропусками библиотечная версия роняет качество до `0.377`. Качество реализованного в рамках работы алгоритма на том же количестве данных составило `0.773`.

Кроме того, библиотечный алгоритм не может работать с категориальными признаками, представленными не в виде чисел. Такие признаки нуждаются в дополнительной предобработке.

На задаче регресии библиотечный аналог показал ошибку MSE = `14818`. Та же ошибка получилась и у разработанного в рамках работы алгоритма.
Библиотечный алгоритм работает быстрее, чем реализованный в рамках работы (15 мс против 5 минут).

**Выводы:**
- Библиотечный алгоритм и реализованный похожи, но работают по-разному. Считать сравнение полностью равнозначным нельзя
- Качество реализованного в рамках работы алгоритма сопоставимо с качеством библиотечного алгоритма
- Реализованный в рамках работы алгоритм гораздо более устойчив к уменьшению обучающей выборки, чем библиотечный
- Реализованный в рамках работы алгоритм, в отличие от библиотечного, может работать с категориальными признаками, представленными не в виде чисел
- Библиотечный алгоритм работает быстрее, чем реализованный в рамках работы.
